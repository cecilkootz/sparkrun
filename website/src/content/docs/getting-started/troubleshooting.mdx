---
title: Tips & Troubleshooting
description: Common issues and practical tips for running sparkrun on DGX Spark systems.
---

## HuggingFace cache owned by root

DGX Spark inference containers typically run as **root** inside rootful Docker. If a container downloads a model from HuggingFace — either intentionally or because a file wasn't pre-cached (e.g., a tokenizer config) — the downloaded files in `~/.cache/huggingface` end up **owned by root**. This causes permission errors on subsequent runs when your non-root user can't read or update the cache.

sparkrun avoids this by downloading models **as your user** on the host and syncing them to cluster nodes before launching containers. But if models were previously downloaded inside a container (or you ran a container manually that pulled a model), you may already have root-owned files.

**Symptoms:**

- `Permission denied` or `OSError: [Errno 13]` errors referencing paths under `~/.cache/huggingface`
- A model that worked before suddenly fails on launch
- sparkrun's model sync step fails with permission errors

**Fix:**

sparkrun has a built-in command that fixes ownership across all cluster hosts in one shot:

```bash
# Fix permissions on your default cluster
sparkrun setup fix-permissions

# Or target specific hosts
sparkrun setup fix-permissions --cluster mylab

# Custom cache location
sparkrun setup fix-permissions --cluster mylab --cache-dir /data/hf-cache

# Preview what would be done
sparkrun setup fix-permissions --cluster mylab --dry-run
```

This runs `chown` on `~/.cache/huggingface` across all target hosts. It tries passwordless sudo first; if that fails, it prompts for a password once.

**Automatic fix on every run:**

sparkrun automatically attempts to fix cache ownership on remote hosts before syncing models. If non-interactive sudo is available (either via general NOPASSWD or a scoped sudoers entry), this happens silently. If sudo isn't available, sparkrun logs a warning and continues — the rsync may still succeed if permissions aren't actually broken.

To make this automatic fix work without ever needing a password, run `--save-sudo` once:

```bash
# Install a scoped sudoers entry on all cluster hosts (one-time setup)
sparkrun setup fix-permissions --cluster mylab --save-sudo
```

This installs a minimal sudoers rule on each host that permits **only** `chown -R <user> <cache_dir>` without a password — no broader sudo privileges are granted. After this, every future `sparkrun run` silently fixes cache ownership before model distribution.

```bash
# Preview what the sudoers entry would look like
sparkrun setup fix-permissions --cluster mylab --save-sudo --dry-run
```

Alternatively, you can fix a single machine manually:

```bash
sudo chown -R $USER:$USER ~/.cache/huggingface
```

**Prevention:**

- Let sparkrun handle model downloading and distribution — it downloads as your user and rsyncs to target nodes.
- Avoid running `huggingface-cli download` or model download scripts inside inference containers. Download on the host instead.
- Run `sparkrun setup fix-permissions --save-sudo` once per cluster so that cache ownership is corrected automatically on every launch.
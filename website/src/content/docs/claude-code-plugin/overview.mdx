---
title: Claude Code Plugin
description: AI-assisted inference management with Claude Code.
---

The sparkrun plugin for [Claude Code](https://claude.ai/code) teaches Claude how to manage LLM inference workloads on your DGX Spark systems.

## What it does

- **Slash commands** — Quick actions for running, stopping, and managing inference jobs
- **Skills** — Detailed reference that Claude uses automatically when working with sparkrun

## Installation

### From the marketplace

```bash
# Add the marketplace (one-time setup)
claude plugin marketplace add scitrera/sparkrun

# Install the plugin
claude plugin install sparkrun@sparkrun
```

## Prerequisites

### sparkrun CLI

```bash
# Install via uvx (recommended)
uvx sparkrun setup install
```

### DGX Spark cluster

You need SSH access to one or more DGX Spark systems:

```bash
sparkrun cluster create mylab --hosts 192.168.11.13,192.168.11.14 -d "My DGX Spark lab"
sparkrun cluster set-default mylab
sparkrun setup ssh --cluster mylab
```

### Claude Code

And you'll obviously need to be using [Claude Code](https://claude.com/product/claude-code).

## Natural language usage

Just describe what you want — Claude uses the skills automatically:

- "Run the Qwen3 1.7B model on my cluster"
- "What inference jobs are running?"
- "Stop the nemotron model"
- "Show me available recipes for llama models"
- "Set up sparkrun on my DGX Spark cluster"

## Key concepts

- **Recipes** are YAML files describing an inference workload (model, runtime, container, defaults)
- **Runtimes** are inference engines: vLLM, SGLang, llama.cpp
- **Clusters** are named groups of DGX Spark hosts
- Each DGX Spark has 1 GPU, so `--tp N` (tensor parallelism) = N hosts
- sparkrun launches detached containers — Ctrl+C detaches from logs, never kills the job
